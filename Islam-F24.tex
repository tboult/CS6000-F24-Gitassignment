\section{Goals for the Course}

As a PhD candidate in Security under the guidance of Dr Sang-Yoon Chang, my primary goal in the "Computer Science Research - CS 6000" course is to deepen my understanding of advanced research methodologies and refine my ability to conduct impactful research in security. This course represents a pivotal opportunity to explore the latest trends, challenges, and innovations in cybersecurity, allowing me to develop a comprehensive framework for addressing complex issues in this domain. Through rigorous analysis and the application of various research techniques, I aim to contribute novel insights to the academic community while also developing practical solutions that can be applied in real-world scenarios.\\

In particular, I am eager to enhance my skills in identifying, analyzing, and solving complex security problems, focusing on areas such as adversarial attacks, secure system design, and the development of robust defence mechanisms. By the end of this course, I hope to have a well-rounded understanding of how to conduct high-quality research that not only advances theoretical knowledge but also has tangible impacts on the security landscape. My ultimate goal is to leverage this knowledge to produce a dissertation that is both academically rigorous and practically significant, contributing to the field of cybersecurity in meaningful ways.\\

Personally, I am married and the father of two energetic boys, both seven years old. Balancing family life with academic pursuits is both challenging and rewarding, and I find that travelling with my family during free time and vacations provides the perfect opportunity to relax and gain new perspectives. My journey from Bangladesh to my current PhD program represents not just a professional ambition but also a personal commitment to growth and learning.

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{images/Amanul_Islam.jpg}
\caption{Amanul Islam}
\label{fig:myphoto}
\end{figure}


\section*{Output of the Training and Validation Loss of Anomaly-Detection-in-Fake-Base-Station-using-Autoencoder: }

I tested the (https://github.com/Luckyaman/Anomaly-Detection-in-Fake-Base-Station-using-Autoencoder.git) related to anomaly detection in 5G networks.

For this project, I tested an autoencoder-based anomaly detection model for 5G networks using a GitHub repository. Over the course of 50 epochs, the model's training and validation losses converged, though fluctuating near similar values. The training process was time-intensive, with some epochs taking over two minutes each. Despite stable loss values, the model ultimately detected 717 anomalies from the dataset. The experience highlighted the need for careful tuning of hyperparameters and further exploration of feature engineering to improve the modelâ€™s performance. It was a valuable learning experience in anomaly detection within telecommunications networks.

 
Epoch 1/50
\texttt{59902/59902 -------------------- 120s 2ms/step - loss: 0.7488 - val\_loss: 0.7521}\\
Epoch 2/50
\texttt{59902/59902 -------------------- 115s 2ms/step - loss: 0.7974 - val\_loss: 0.7516}\\
Epoch 3/50
\texttt{59902/59902 -------------------- 122s 2ms/step - loss: 0.6965 - val\_loss: 0.7520}\\
Epoch 4/50
\texttt{59902/59902 -------------------- 124s 2ms/step - loss: 0.8732 - val\_loss: 0.7520}\\
Epoch 5/50
\texttt{59902/59902 -------------------- 129s 2ms/step - loss: 0.7421 - val\_loss: 0.7518}\\
Epoch 6/50
\texttt{59902/59902 -------------------- 128s 2ms/step - loss: 0.9766 - val\_loss: 0.7513}\\
Epoch 7/50
\texttt{59902/59902 -------------------- 136s 2ms/step - loss: 0.7514 - val\_loss: 0.7513}\\
Epoch 8/50
\texttt{59902/59902 -------------------- 125s 2ms/step - loss: 0.9412 - val\_loss: 0.7512}\\
Epoch 9/50
\texttt{59902/59902 -------------------- 126s 2ms/step - loss: 0.7056 - val\_loss: 0.7512}\\
Epoch 10/50
\texttt{59902/59902 -------------------- 145s 2ms/step - loss: 0.8189 - val\_loss: 0.7518}\\
Epoch 11/50
\texttt{59902/59902 -------------------- 134s 2ms/step - loss: 0.8691 - val\_loss: 0.7512}\\
Epoch 12/50
\texttt{59902/59902 -------------------- 121s 2ms/step - loss: 0.7325 - val\_loss: 0.7512}\\
Epoch 13/50
\texttt{59902/59902 -------------------- 120s 2ms/step - loss: 0.7405 - val\_loss: 0.7519}\\
Epoch 14/50
\texttt{59902/59902 -------------------- 124s 2ms/step - loss: 0.8846 - val\_loss: 0.7517}\\
Epoch 15/50
\texttt{59902/59902 -------------------- 166s 2ms/step - loss: 0.8146 - val\_loss: 0.7517}\\
Epoch 16/50
\texttt{59902/59902 -------------------- 191s 2ms/step - loss: 0.8228 - val\_loss: 0.7512}\\
Epoch 17/50
\texttt{59902/59902 -------------------- 148s 2ms/step - loss: 0.8338 - val\_loss: 0.7512}\\
Epoch 18/50
\texttt{59902/59902 -------------------- 219s 3ms/step - loss: 0.9288 - val\_loss: 0.7512}\\
Epoch 19/50
\texttt{59902/59902 -------------------- 182s 2ms/step - loss: 0.8441 - val\_loss: 0.7512}\\
Epoch 20/50
\texttt{59902/59902 -------------------- 127s 2ms/step - loss: 0.8198 - val\_loss: 0.7512}\\
Epoch 21/50
\texttt{59902/59902 -------------------- 126s 2ms/step - loss: 0.7595 - val\_loss: 0.7516}\\
Epoch 22/50
\texttt{59902/59902 -------------------- 139s 2ms/step - loss: 0.9608 - val\_loss: 0.7518}\\
Epoch 23/50
\texttt{59902/59902 -------------------- 122s 2ms/step - loss: 0.8043 - val\_loss: 0.7512}\\
Epoch 24/50
\texttt{59902/59902 -------------------- 144s 2ms/step - loss: 0.8094 - val\_loss: 0.7523}\\
Epoch 25/50
\texttt{59902/59902 -------------------- 143s 2ms/step - loss: 0.8531 - val\_loss: 0.7512}\\
Epoch 26/50
\texttt{59902/59902 -------------------- 121s 2ms/step - loss: 0.7947 - val\_loss: 0.7511}\\
Epoch 27/50
\texttt{59902/59902 -------------------- 123s 2ms/step - loss: 0.7315 - val\_loss: 0.7512}\\
Epoch 28/50
\texttt{59902/59902 -------------------- 122s 2ms/step - loss: 0.8460 - val\_loss: 0.7512}\\
Epoch 29/50
\texttt{59902/59902 -------------------- 116s 2ms/step - loss: 0.7275 - val\_loss: 0.7517}\\
Epoch 30/50
\texttt{59902/59902 -------------------- 137s 2ms/step - loss: 0.8481 - val\_loss: 0.7512}\\
Epoch 31/50
\texttt{59902/59902 -------------------- 144s 2ms/step - loss: 0.8883 - val\_loss: 0.7512}\\
Epoch 32/50
\texttt{59902/59902 -------------------- 111s 2ms/step - loss: 0.9098 - val\_loss: 0.7512}\\
Epoch 33/50
\texttt{59902/59902 -------------------- 111s 2ms/step - loss: 0.8350 - val\_loss: 0.7512}\\
Epoch 34/50
\texttt{59902/59902 -------------------- 145s 2ms/step - loss: 0.7062 - val\_loss: 0.7512}\\
Epoch 35/50
\texttt{59902/59902 -------------------- 139s 2ms/step - loss: 0.8112 - val\_loss: 0.7511}\\
Epoch 36/50
\texttt{59902/59902 -------------------- 152s 2ms/step - loss: 1.0590 - val\_loss: 0.7513}\\
Epoch 37/50
\texttt{59902/59902 -------------------- 123s 2ms/step - loss: 0.7585 - val\_loss: 0.7512}\\
Epoch 38/50
\texttt{59902/59902 -------------------- 139s 2ms/step - loss: 0.7457 - val\_loss: 0.7517}\\
Epoch 39/50
\texttt{59902/59902 -------------------- 136s 2ms/step - loss: 0.7591 - val\_loss: 0.7517}\\
Epoch 40/50
\texttt{59902/59902 -------------------- 134s 2ms/step - loss: 0.7131 - val\_loss: 0.7512}\\
Epoch 41/50
\texttt{59902/59902 -------------------- 143s 2ms/step - loss: 0.8032 - val\_loss: 0.7511}\\
Epoch 42/50
\texttt{59902/59902 -------------------- 133s 2ms/step - loss: 0.7928 - val\_loss: 0.7512}\\
Epoch 43/50
\texttt{59902/59902 -------------------- 150s 2ms/step - loss: 0.8165 - val\_loss: 0.7512}\\
Epoch 44/50
\texttt{59902/59902 -------------------- 136s 2ms/step - loss: 1.1533 - val\_loss: 0.7517}\\
Epoch 45/50
\texttt{59902/59902 -------------------- 148s 2ms/step - loss: 0.7264 - val\_loss: 0.7512}\\
Epoch 46/50
\texttt{59902/59902 -------------------- 141s 2ms/step - loss: 0.8815 - val\_loss: 0.7511}\\
Epoch 47/50
\texttt{59902/59902 -------------------- 146s 2ms/step - loss: 0.7979 - val\_loss: 0.7514}\\
Epoch 48/50
\texttt{59902/59902 -------------------- 211s 3ms/step - loss: 0.8380 - val\_loss: 0.7511}\\
Epoch 49/50
\texttt{59902/59902 -------------------- 152s 3ms/step - loss: 0.7694 - val\_loss: 0.7511}\\
Epoch 50/50
\texttt{59902/59902 -------------------- 200s 3ms/step - loss: 0.7637 - val\_loss: 0.7511}\\
74877/74877 -------------------- 130s 2ms/step\\
Number of anomalies detected: 717

\section*{Questions: }
What activation functions and optimiser did you use?(From Sahil)

The ReLU  functions are used in the hidden layers of the encoder to capture non-linear patterns for anomaly detection in fake base stations using autoencoders. In the decoder, a sigmoid activation is  applied for normalized data. The Adam optimizer is  used due to its efficient convergence and ability to adapt learning rates. This helps  accurately reconstruct normal data while highlighting anomalies like fake base stations.

Have you considered looking at any encryption algorithms like quantum algorithms? DC(From Abdul)

I haven't specifically considered quantum encryption algorithms in my current research on fake base station detection and anomaly detection, as the focus has been more on machine learning methods for detecting malicious behaviour. However, quantum encryption algorithms, such as quantum key distribution (QKD), could play a crucial role in securing communication channels against fake base stations in the future. 

\subsection{Questions from Aaron McKay}
1. In your experience testing the anomaly detection model, what factors do you think contributed to detecting exactly 717 anomalies, and how would you validate if these were true positives?

In my experience testing the anomaly detection model, I can identify a few reasons for catching exactly 717 anomalies. First, the choice of the threshold for reconstruction error is important, as it defines the boundary between normal and anomalous behaviour. A well-tuned threshold ensures that only significant deviations are flagged as anomalies. Second, the quality and diversity of the training data play a major role. If the model was trained primarily on normal data, it would be sensitive to deviations, such as those caused by fake base stations. Additionally, data preprocessing, including normalization and noise reduction, ensures the model learns meaningful patterns without being affected by outliers.
To validate if these 717 anomalies are true positives, I would cross-validate the model using a labelled dataset where the anomalies (fake base stations) are known. This would help confirm whether the detected anomalies match the ground truth. I would conduct a manual investigation of the flagged instances to check if they exhibit characteristics of fake base stations, such as abnormal time delays or unusual transaction behaviour.

2. As a PhD candidate in Security working with Dr. Chang, how do you see your research in adversarial attacks potentially intersecting with fake base station detection?

As a PhD candidate in Security working with Dr. Chang, I see my research in adversarial attacks intersecting with fake base station detection in several ways. Adversarial attacks involve manipulating input data to deceive machine learning models, and in the context of fake base stations, attackers could exploit vulnerabilities in anomaly detection models by subtly altering the behaviour of malicious base stations to make them appear legitimate. This presents a challenge in distinguishing between true anomalies and adversarially manipulated data. My research could focus on enhancing the robustness of detection models against such attacks by incorporating adversarial training or defensive techniques like robust optimization. This would strengthen the model's ability to not only detect traditional fake base stations but also identify more sophisticated threats that attempt to bypass detection through adversarial methods.
