\section{Goals for the Course}

As a PhD candidate in Security under the guidance of Dr Sang-Yoon Chang, my primary goal in the "Computer Science Research - CS 6000" course is to deepen my understanding of advanced research methodologies and refine my ability to conduct impactful research in security. This course represents a pivotal opportunity to explore the latest trends, challenges, and innovations in cybersecurity, allowing me to develop a comprehensive framework for addressing complex issues in this domain. Through rigorous analysis and the application of various research techniques, I aim to contribute novel insights to the academic community while also developing practical solutions that can be applied in real-world scenarios.\\

In particular, I am eager to enhance my skills in identifying, analyzing, and solving complex security problems, focusing on areas such as adversarial attacks, secure system design, and the development of robust defence mechanisms. By the end of this course, I hope to have a well-rounded understanding of how to conduct high-quality research that not only advances theoretical knowledge but also has tangible impacts on the security landscape. My ultimate goal is to leverage this knowledge to produce a dissertation that is both academically rigorous and practically significant, contributing to the field of cybersecurity in meaningful ways.\\

Personally, I am married and the father of two energetic boys, both seven years old. Balancing family life with academic pursuits is both challenging and rewarding, and I find that travelling with my family during free time and vacations provides the perfect opportunity to relax and gain new perspectives. My journey from Bangladesh to my current PhD program represents not just a professional ambition but also a personal commitment to growth and learning.

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{images/Amanul_Islam.jpg}
\caption{Amanul Islam}
\label{fig:myphoto}
\end{figure}


\section*{Output of the Training and Validation Loss of Anomaly-Detection-in-Fake-Base-Station-using-Autoencoder: }

I tested the (https://github.com/Luckyaman/Anomaly-Detection-in-Fake-Base-Station-using-Autoencoder.git) related to anomaly detection in 5G networks.

For this project, I tested an autoencoder-based anomaly detection model for 5G networks using a GitHub repository. Over the course of 50 epochs, the model's training and validation losses converged, though fluctuating near similar values. The training process was time-intensive, with some epochs taking over two minutes each. Despite stable loss values, the model ultimately detected 717 anomalies from the dataset. The experience highlighted the need for careful tuning of hyperparameters and further exploration of feature engineering to improve the model’s performance. It was a valuable learning experience in anomaly detection within telecommunications networks.

Epoch 1/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 120s 2ms/step - loss: 0.7488 - val_loss: 0.7521
Epoch 2/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 115s 2ms/step - loss: 0.7974 - val_loss: 0.7516
Epoch 3/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 122s 2ms/step - loss: 0.6965 - val_loss: 0.7520
Epoch 4/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 124s 2ms/step - loss: 0.8732 - val_loss: 0.7520
Epoch 5/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 129s 2ms/step - loss: 0.7421 - val_loss: 0.7518
Epoch 6/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 128s 2ms/step - loss: 0.9766 - val_loss: 0.7513
Epoch 7/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 136s 2ms/step - loss: 0.7514 - val_loss: 0.7513
Epoch 8/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 125s 2ms/step - loss: 0.9412 - val_loss: 0.7512
Epoch 9/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 126s 2ms/step - loss: 0.7056 - val_loss: 0.7512
Epoch 10/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 145s 2ms/step - loss: 0.8189 - val_loss: 0.7518
Epoch 11/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 134s 2ms/step - loss: 0.8691 - val_loss: 0.7512
Epoch 12/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 121s 2ms/step - loss: 0.7325 - val_loss: 0.7512
Epoch 13/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 120s 2ms/step - loss: 0.7405 - val_loss: 0.7519
Epoch 14/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 124s 2ms/step - loss: 0.8846 - val_loss: 0.7517
Epoch 15/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 166s 2ms/step - loss: 0.8146 - val_loss: 0.7517
Epoch 16/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 191s 2ms/step - loss: 0.8228 - val_loss: 0.7512
Epoch 17/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 148s 2ms/step - loss: 0.8338 - val_loss: 0.7512
Epoch 18/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 219s 3ms/step - loss: 0.9288 - val_loss: 0.7512
Epoch 19/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 182s 2ms/step - loss: 0.8441 - val_loss: 0.7512
Epoch 20/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 127s 2ms/step - loss: 0.8198 - val_loss: 0.7512
Epoch 21/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 126s 2ms/step - loss: 0.7595 - val_loss: 0.7516
Epoch 22/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 139s 2ms/step - loss: 0.9608 - val_loss: 0.7518
Epoch 23/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 122s 2ms/step - loss: 0.8043 - val_loss: 0.7512
Epoch 24/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 144s 2ms/step - loss: 0.8094 - val_loss: 0.7523
Epoch 25/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 143s 2ms/step - loss: 0.8531 - val_loss: 0.7512
Epoch 26/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 121s 2ms/step - loss: 0.7947 - val_loss: 0.7511
Epoch 27/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 123s 2ms/step - loss: 0.7315 - val_loss: 0.7512
Epoch 28/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 122s 2ms/step - loss: 0.8460 - val_loss: 0.7512
Epoch 29/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 116s 2ms/step - loss: 0.7275 - val_loss: 0.7517
Epoch 30/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 137s 2ms/step - loss: 0.8481 - val_loss: 0.7512
Epoch 31/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 144s 2ms/step - loss: 0.8883 - val_loss: 0.7512
Epoch 32/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 111s 2ms/step - loss: 0.9098 - val_loss: 0.7512
Epoch 33/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 111s 2ms/step - loss: 0.8350 - val_loss: 0.7512
Epoch 34/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 145s 2ms/step - loss: 0.7062 - val_loss: 0.7512
Epoch 35/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 139s 2ms/step - loss: 0.8112 - val_loss: 0.7511
Epoch 36/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 152s 2ms/step - loss: 1.0590 - val_loss: 0.7513
Epoch 37/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 123s 2ms/step - loss: 0.7585 - val_loss: 0.7512
Epoch 38/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 139s 2ms/step - loss: 0.7457 - val_loss: 0.7517
Epoch 39/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 136s 2ms/step - loss: 0.7591 - val_loss: 0.7517
Epoch 40/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 134s 2ms/step - loss: 0.7131 - val_loss: 0.7512
Epoch 41/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 143s 2ms/step - loss: 0.8032 - val_loss: 0.7511
Epoch 42/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 133s 2ms/step - loss: 0.7928 - val_loss: 0.7512
Epoch 43/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 150s 2ms/step - loss: 0.8165 - val_loss: 0.7512
Epoch 44/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 136s 2ms/step - loss: 1.1533 - val_loss: 0.7517
Epoch 45/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 148s 2ms/step - loss: 0.7264 - val_loss: 0.7512
Epoch 46/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 141s 2ms/step - loss: 0.8815 - val_loss: 0.7511
Epoch 47/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 146s 2ms/step - loss: 0.7979 - val_loss: 0.7514
Epoch 48/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 211s 3ms/step - loss: 0.8380 - val_loss: 0.7511
Epoch 49/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 152s 3ms/step - loss: 0.7694 - val_loss: 0.7511
Epoch 50/50
59902/59902 ━━━━━━━━━━━━━━━━━━━━ 200s 3ms/step - loss: 0.7637 - val_loss: 0.7511
74877/74877 ━━━━━━━━━━━━━━━━━━━━ 130s 2ms/step
Number of anomalies detected: 717

\section*{Questions: }
What activation functions and optimiser did you use? (From Sahil Thapa)
Answer: For anomaly detection in fake base stations using autoencoders, the ReLU or Leaky ReLU activation functions are  used in the hidden layers of the encoder to capture non-linear patterns. In the decoder, a sigmoid activation is  applied for normalized data, while a linear activation is used for continuous outputs. The Adam optimizer is widely used due to its efficient convergence and ability to adapt learning rates, making it ideal for training models in anomaly detection tasks. This combination helps in accurately reconstructing normal data while highlighting anomalies like fake base stations.

\subsection{Questions from Aaron McKay}
1. In your experience testing the anomaly detection model, what factors do you think contributed to detecting exactly 717 anomalies, and how would you validate if these were true positives?
In my experience testing the anomaly detection model, several factors contributed to detecting exactly 717 anomalies. First, the choice of the threshold for reconstruction error is crucial, as it defines the boundary between normal and anomalous behaviour. A well-tuned threshold ensures that only significant deviations are flagged as anomalies. Second, the quality and diversity of the training data play a major role. If the model was trained primarily on normal data, it would be sensitive to deviations, such as those caused by fake base stations. Additionally, data preprocessing, including normalization and noise reduction, ensures the model learns meaningful patterns without being affected by outliers.
To validate if these 717 anomalies are true positives, I would:
1. Cross-validate the model using a labelled dataset where the anomalies (fake base stations) are known. This would help confirm whether the detected anomalies match the ground truth.
2. Conduct a manual investigation of the flagged instances to check if they exhibit characteristics of fake base stations, such as abnormal time delays or unusual transaction behaviour.
3. Use domain-specific tests, such as checking for specific features in fake base stations (e.g., signal characteristics), to ensure that the anomalies correspond to actual threats and not benign deviations.

2. As a PhD candidate in Security working with Dr. Chang, how do you see your research in adversarial attacks potentially intersecting with fake base station detection?
As a PhD candidate in Security working with Dr. Chang, I see my research in adversarial attacks intersecting with fake base station detection in several ways. Adversarial attacks involve manipulating input data to deceive machine learning models, and in the context of fake base stations, attackers could exploit vulnerabilities in anomaly detection models by subtly altering the behaviour of malicious base stations to make them appear legitimate. This presents a challenge in distinguishing between true anomalies and adversarially manipulated data. My research could focus on enhancing the robustness of detection models against such attacks by incorporating adversarial training or defensive techniques like robust optimization. This would strengthen the model's ability to not only detect traditional fake base stations but also identify more sophisticated threats that attempt to bypass detection through adversarial methods.
